digraph {
	graph [size="18.45,18.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2129022630800 [label="
 (5196, 8)" fillcolor=darkolivegreen1]
	2129022759264 [label=AddmmBackward0]
	2129022759120 -> 2129022759264
	2127987594672 [label="layers.17.bias
 (8)" fillcolor=lightblue]
	2127987594672 -> 2129022759120
	2129022759120 [label=AccumulateGrad]
	2129022759072 -> 2129022759264
	2129022759072 [label=LeakyReluBackward0]
	2129022758976 -> 2129022759072
	2129022758976 [label=NativeBatchNormBackward0]
	2129022758832 -> 2129022758976
	2129022758832 [label=AddmmBackward0]
	2129022758496 -> 2129022758832
	2127987598320 [label="layers.14.bias
 (128)" fillcolor=lightblue]
	2127987598320 -> 2129022758496
	2129022758496 [label=AccumulateGrad]
	2129022758544 -> 2129022758832
	2129022758544 [label=LeakyReluBackward0]
	2129022758400 -> 2129022758544
	2129022758400 [label=NativeBatchNormBackward0]
	2129022758112 -> 2129022758400
	2129022758112 [label=AddmmBackward0]
	2129022757872 -> 2129022758112
	2127987591792 [label="layers.11.bias
 (256)" fillcolor=lightblue]
	2127987591792 -> 2129022757872
	2129022757872 [label=AccumulateGrad]
	2129022757968 -> 2129022758112
	2129022757968 [label=LeakyReluBackward0]
	2129022757728 -> 2129022757968
	2129022757728 [label=NativeBatchNormBackward0]
	2129022759840 -> 2129022757728
	2129022759840 [label=AddmmBackward0]
	2129022760032 -> 2129022759840
	2127983699120 [label="layers.7.bias
 (512)" fillcolor=lightblue]
	2127983699120 -> 2129022760032
	2129022760032 [label=AccumulateGrad]
	2129022759984 -> 2129022759840
	2129022759984 [label=LeakyReluBackward0]
	2129022760128 -> 2129022759984
	2129022760128 [label=AddmmBackward0]
	2129022760320 -> 2129022760128
	2127983694320 [label="layers.4.bias
 (1024)" fillcolor=lightblue]
	2127983694320 -> 2129022760320
	2129022760320 [label=AccumulateGrad]
	2129022760272 -> 2129022760128
	2129022760272 [label=LeakyReluBackward0]
	2129022760416 -> 2129022760272
	2129022760416 [label=AddmmBackward0]
	2129022760608 -> 2129022760416
	2127983692784 [label="layers.1.bias
 (2048)" fillcolor=lightblue]
	2127983692784 -> 2129022760608
	2129022760608 [label=AccumulateGrad]
	2129022760560 -> 2129022760416
	2129022760560 [label=NativeBatchNormBackward0]
	2129022760704 -> 2129022760560
	2127983694896 [label="layers.0.weight
 (1185)" fillcolor=lightblue]
	2127983694896 -> 2129022760704
	2129022760704 [label=AccumulateGrad]
	2129022760752 -> 2129022760560
	2127983693264 [label="layers.0.bias
 (1185)" fillcolor=lightblue]
	2127983693264 -> 2129022760752
	2129022760752 [label=AccumulateGrad]
	2129022760512 -> 2129022760416
	2129022760512 [label=TBackward0]
	2129022760896 -> 2129022760512
	2127983692880 [label="layers.1.weight
 (2048, 1185)" fillcolor=lightblue]
	2127983692880 -> 2129022760896
	2129022760896 [label=AccumulateGrad]
	2129022760224 -> 2129022760128
	2129022760224 [label=TBackward0]
	2129022760848 -> 2129022760224
	2127983692688 [label="layers.4.weight
 (1024, 2048)" fillcolor=lightblue]
	2127983692688 -> 2129022760848
	2129022760848 [label=AccumulateGrad]
	2129022759936 -> 2129022759840
	2129022759936 [label=TBackward0]
	2129022760656 -> 2129022759936
	2127983695856 [label="layers.7.weight
 (512, 1024)" fillcolor=lightblue]
	2127983695856 -> 2129022760656
	2129022760656 [label=AccumulateGrad]
	2129022759792 -> 2129022757728
	2127983701808 [label="layers.8.weight
 (512)" fillcolor=lightblue]
	2127983701808 -> 2129022759792
	2129022759792 [label=AccumulateGrad]
	2129022759744 -> 2129022757728
	2127983701520 [label="layers.8.bias
 (512)" fillcolor=lightblue]
	2127983701520 -> 2129022759744
	2129022759744 [label=AccumulateGrad]
	2129022758016 -> 2129022758112
	2129022758016 [label=TBackward0]
	2129022760080 -> 2129022758016
	2127979365776 [label="layers.11.weight
 (256, 512)" fillcolor=lightblue]
	2127979365776 -> 2129022760080
	2129022760080 [label=AccumulateGrad]
	2129022758256 -> 2129022758400
	2127987596112 [label="layers.12.weight
 (256)" fillcolor=lightblue]
	2127987596112 -> 2129022758256
	2129022758256 [label=AccumulateGrad]
	2129022758304 -> 2129022758400
	2127987594192 [label="layers.12.bias
 (256)" fillcolor=lightblue]
	2127987594192 -> 2129022758304
	2129022758304 [label=AccumulateGrad]
	2129022758592 -> 2129022758832
	2129022758592 [label=TBackward0]
	2129022757824 -> 2129022758592
	2127987595152 [label="layers.14.weight
 (128, 256)" fillcolor=lightblue]
	2127987595152 -> 2129022757824
	2129022757824 [label=AccumulateGrad]
	2129022758880 -> 2129022758976
	2127987600720 [label="layers.15.weight
 (128)" fillcolor=lightblue]
	2127987600720 -> 2129022758880
	2129022758880 [label=AccumulateGrad]
	2129022758928 -> 2129022758976
	2127987592944 [label="layers.15.bias
 (128)" fillcolor=lightblue]
	2127987592944 -> 2129022758928
	2129022758928 [label=AccumulateGrad]
	2129022759024 -> 2129022759264
	2129022759024 [label=TBackward0]
	2129022758448 -> 2129022759024
	2127987593232 [label="layers.17.weight
 (8, 128)" fillcolor=lightblue]
	2127987593232 -> 2129022758448
	2129022758448 [label=AccumulateGrad]
	2129022759264 -> 2129022630800
}

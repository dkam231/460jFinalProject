{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff35e69a-df42-4088-ac93-04858cf52844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe4dc82-0049-4b02-a9b2-8d6c75a75b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\"\n",
    "# C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_metadata/fma_metadata/echonest.csv\n",
    "file_path=\"C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small/000/000002.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94fbcd1-d393-4e5b-b9a3-87692ace4e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Get the Mel-frequency cepstral coefficients\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfccs_processed = np.mean(mfcc.T,axis=0)\n",
    "        \n",
    "        # Get the spectral contrast\n",
    "        stft = np.abs(librosa.stft(y))\n",
    "        contrast = librosa.feature.spectral_contrast(S=stft, sr=sr)\n",
    "        contrast_processed = np.mean(contrast.T,axis=0)\n",
    "        \n",
    "        # Get the chroma feature\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma_processed = np.mean(chroma.T,axis=0)\n",
    "        \n",
    "        # Get the mel-scaled spectrogram\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "        mel_processed = np.mean(mel.T,axis=0)\n",
    "\n",
    "        # Concatenate all features into one array\n",
    "        features = np.hstack([mfccs_processed, contrast_processed, chroma_processed, mel_processed])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error encountered while parsing file: {file_path}, error: {e}\")\n",
    "        return None \n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abe0fb1c-189b-433b-9843-60ea98362117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_audio_files(base_directory):\n",
    "    features_list = []\n",
    "    track_ids = []\n",
    "\n",
    "    # Walk through all files in the directory structure\n",
    "    for subdir, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                features = extract_features(file_path)\n",
    "                if features is not None:\n",
    "                    features_list.append(features)\n",
    "                    # Extract track ID from filename\n",
    "                    track_id = int(file.split('.')[0])\n",
    "                    track_ids.append(track_id)\n",
    "\n",
    "    return features_list, track_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6862c6cf-bd82-43bb-ac1b-ec4174046bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaw\\AppData\\Local\\Temp\\ipykernel_79176\\538791090.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(file_path, sr=None)\n",
      "C:\\Users\\annaw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\098\\098565.mp3, error: \n",
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\098\\098567.mp3, error: \n",
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\098\\098569.mp3, error: \n",
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\099\\099134.mp3, error: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\108\\108925.mp3, error: \n",
      "Error encountered while parsing file: C:/Users/annaw/Python Projects/jupyterNotebooks/DSL Final Project/fma_small/fma_small\\133\\133297.mp3, error: \n"
     ]
    }
   ],
   "source": [
    "# Process all audio files and collect features\n",
    "features, track_ids = process_all_audio_files(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18d01a0-ce3e-4c6c-a887-c5f5a8d5194c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a593b7e09f3749478eb06bd97a82074b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annaw\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\annaw\\.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da676e2e7db24c9282b0a3e15794d640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3354ad38b143b783115eb7eaed1ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e8b5e7205b4b00b106786c4bd8c838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605b5f15e8484f39be9f0d0373ffc8b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Initialize Wav2Vec2 processor and feature extractor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "\n",
    "def extract_wav2vec_features(audio_file):\n",
    "    audio_input, _ = feature_extractor(audio_file, return_tensors=\"pt\", padding=\"longest\")\n",
    "    with torch.no_grad():\n",
    "        features = processor.extract_features(audio_input.input_values)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c580c13-2045-449f-b8f8-fe28b8f05df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all audio files and collect features and labels\n",
    "# Convert features and labels to DataFrame\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df['track_ids'] = track_ids\n",
    "\n",
    "# Save features to CSV\n",
    "features_df.to_csv('audio_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2efccd09-f9e5-4eef-b853-0575778a877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2FeatureExtractor\n",
    "\n",
    "# Initialize Wav2Vec2 processor and feature extractor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Implement audio preprocessing if required (e.g., resampling)\n",
    "    # Return the preprocessed audio waveform\n",
    "    pass\n",
    "\n",
    "def extract_wav2vec_features(audio_file):\n",
    "    audio_input, _ = feature_extractor(audio_file, return_tensors=\"pt\", padding=\"longest\")\n",
    "    with torch.no_grad():\n",
    "        features = processor.extract_features(audio_input.input_values)\n",
    "    return features\n",
    "\n",
    "def process_all_audio_files(base_directory):\n",
    "    features_list = []\n",
    "    labels = []\n",
    "\n",
    "    for subdir, dirs, files in os.walk(base_directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                audio_file = preprocess_audio(file_path)\n",
    "                if audio_file is not None:\n",
    "                    features = extract_wav2vec_features(audio_file)\n",
    "                    features_list.append(features.numpy())\n",
    "                    labels.append(subdir.split('/')[-1])  # Assuming folder name is the label\n",
    "\n",
    "    return features_list, labels\n",
    "\n",
    "# Process all audio files and collect features and labels\n",
    "features, labels = process_all_audio_files(data_dir)\n",
    "\n",
    "# Convert features and labels to DataFrame\n",
    "features_df = pd.DataFrame(features)\n",
    "features_df['label'] = labels\n",
    "\n",
    "# Save features to CSV\n",
    "features_df.to_csv('audio_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ec680-3f82-4637-bb62-83409512df3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

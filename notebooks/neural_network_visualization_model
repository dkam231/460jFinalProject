digraph {
	graph [size="19.95,19.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1592545142416 [label="
 (1, 8)" fillcolor=darkolivegreen1]
	1591421730096 [label=AddmmBackward0]
	1591421722032 -> 1591421730096
	1592543414224 [label="layers.18.bias
 (8)" fillcolor=lightblue]
	1592543414224 -> 1591421722032
	1591421722032 [label=AccumulateGrad]
	1591545316496 -> 1591421730096
	1591545316496 [label=LeakyReluBackward0]
	1591545323792 -> 1591545316496
	1591545323792 [label=NativeBatchNormBackward0]
	1591545319664 -> 1591545323792
	1591545319664 [label=AddmmBackward0]
	1591545326240 -> 1591545319664
	1591379941456 [label="layers.15.bias
 (128)" fillcolor=lightblue]
	1591379941456 -> 1591545326240
	1591545326240 [label=AccumulateGrad]
	1591545327248 -> 1591545319664
	1591545327248 [label=LeakyReluBackward0]
	1591545327680 -> 1591545327248
	1591545327680 [label=NativeBatchNormBackward0]
	1591545331424 -> 1591545327680
	1591545331424 [label=AddmmBackward0]
	1591545316688 -> 1591545331424
	1591379949232 [label="layers.12.bias
 (256)" fillcolor=lightblue]
	1591379949232 -> 1591545316688
	1591545316688 [label=AccumulateGrad]
	1591545321968 -> 1591545331424
	1591545321968 [label=LeakyReluBackward0]
	1591545317264 -> 1591545321968
	1591545317264 [label=NativeBatchNormBackward0]
	1591545320480 -> 1591545317264
	1591545320480 [label=AddmmBackward0]
	1591545318032 -> 1591545320480
	1591379948176 [label="layers.8.bias
 (512)" fillcolor=lightblue]
	1591379948176 -> 1591545318032
	1591545318032 [label=AccumulateGrad]
	1591545320576 -> 1591545320480
	1591545320576 [label=LeakyReluBackward0]
	1591545322304 -> 1591545320576
	1591545322304 [label=AddmmBackward0]
	1591545325712 -> 1591545322304
	1591379941552 [label="layers.5.bias
 (1024)" fillcolor=lightblue]
	1591379941552 -> 1591545325712
	1591545325712 [label=AccumulateGrad]
	1591545325856 -> 1591545322304
	1591545325856 [label=LeakyReluBackward0]
	1591545326720 -> 1591545325856
	1591545326720 [label=NativeBatchNormBackward0]
	1591545319472 -> 1591545326720
	1591545319472 [label=AddmmBackward0]
	1591545318272 -> 1591545319472
	1589931435472 [label="layers.1.bias
 (2048)" fillcolor=lightblue]
	1589931435472 -> 1591545318272
	1591545318272 [label=AccumulateGrad]
	1591545322592 -> 1591545319472
	1591545322592 [label=NativeBatchNormBackward0]
	1591545323360 -> 1591545322592
	1591379103280 [label="layers.0.weight
 (1185)" fillcolor=lightblue]
	1591379103280 -> 1591545323360
	1591545323360 [label=AccumulateGrad]
	1591545324560 -> 1591545322592
	1591379108368 [label="layers.0.bias
 (1185)" fillcolor=lightblue]
	1591379108368 -> 1591545324560
	1591545324560 [label=AccumulateGrad]
	1591545322400 -> 1591545319472
	1591545322400 [label=TBackward0]
	1591545329696 -> 1591545322400
	1589931427984 [label="layers.1.weight
 (2048, 1185)" fillcolor=lightblue]
	1589931427984 -> 1591545329696
	1591545329696 [label=AccumulateGrad]
	1591545320864 -> 1591545326720
	1589931430288 [label="layers.2.weight
 (2048)" fillcolor=lightblue]
	1589931430288 -> 1591545320864
	1591545320864 [label=AccumulateGrad]
	1591545322640 -> 1591545326720
	1589931438352 [label="layers.2.bias
 (2048)" fillcolor=lightblue]
	1589931438352 -> 1591545322640
	1591545322640 [label=AccumulateGrad]
	1591545319568 -> 1591545322304
	1591545319568 [label=TBackward0]
	1591545326048 -> 1591545319568
	1591379758256 [label="layers.5.weight
 (1024, 2048)" fillcolor=lightblue]
	1591379758256 -> 1591545326048
	1591545326048 [label=AccumulateGrad]
	1591545318080 -> 1591545320480
	1591545318080 [label=TBackward0]
	1591545323264 -> 1591545318080
	1591379950096 [label="layers.8.weight
 (512, 1024)" fillcolor=lightblue]
	1591379950096 -> 1591545323264
	1591545323264 [label=AccumulateGrad]
	1591545320768 -> 1591545317264
	1591379941360 [label="layers.9.weight
 (512)" fillcolor=lightblue]
	1591379941360 -> 1591545320768
	1591545320768 [label=AccumulateGrad]
	1591545325424 -> 1591545317264
	1591379949904 [label="layers.9.bias
 (512)" fillcolor=lightblue]
	1591379949904 -> 1591545325424
	1591545325424 [label=AccumulateGrad]
	1591545329984 -> 1591545331424
	1591545329984 [label=TBackward0]
	1591545320096 -> 1591545329984
	1591379950960 [label="layers.12.weight
 (256, 512)" fillcolor=lightblue]
	1591379950960 -> 1591545320096
	1591545320096 [label=AccumulateGrad]
	1591545323312 -> 1591545327680
	1591379947696 [label="layers.13.weight
 (256)" fillcolor=lightblue]
	1591379947696 -> 1591545323312
	1591545323312 [label=AccumulateGrad]
	1591545326000 -> 1591545327680
	1591379949328 [label="layers.13.bias
 (256)" fillcolor=lightblue]
	1591379949328 -> 1591545326000
	1591545326000 [label=AccumulateGrad]
	1591545323936 -> 1591545319664
	1591545323936 [label=TBackward0]
	1591545325232 -> 1591545323936
	1591379944720 [label="layers.15.weight
 (128, 256)" fillcolor=lightblue]
	1591379944720 -> 1591545325232
	1591545325232 [label=AccumulateGrad]
	1591545316544 -> 1591545323792
	1591379945200 [label="layers.16.weight
 (128)" fillcolor=lightblue]
	1591379945200 -> 1591545316544
	1591545316544 [label=AccumulateGrad]
	1591545321104 -> 1591545323792
	1591379948464 [label="layers.16.bias
 (128)" fillcolor=lightblue]
	1591379948464 -> 1591545321104
	1591545321104 [label=AccumulateGrad]
	1591545327536 -> 1591421730096
	1591545327536 [label=TBackward0]
	1591545324416 -> 1591545327536
	1592543417296 [label="layers.18.weight
 (8, 128)" fillcolor=lightblue]
	1592543417296 -> 1591545324416
	1591545324416 [label=AccumulateGrad]
	1591421730096 -> 1592545142416
}

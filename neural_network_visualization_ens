digraph {
	graph [size="40.949999999999996,40.949999999999996"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1607677603248 [label="
 (1, 8)" fillcolor=darkolivegreen1]
	1607677617360 [label=AddmmBackward0]
	1607677610736 -> 1607677617360
	1607675498128 [label="fc.bias
 (8)" fillcolor=lightblue]
	1607675498128 -> 1607677610736
	1607677610736 [label=AccumulateGrad]
	1607677615968 -> 1607677617360
	1607677615968 [label=CatBackward0]
	1607676899248 -> 1607677615968
	1607676899248 [label=AddmmBackward0]
	1607676902224 -> 1607676899248
	1607675501200 [label="mlp.layers.17.bias
 (8)" fillcolor=lightblue]
	1607675501200 -> 1607676902224
	1607676902224 [label=AccumulateGrad]
	1607676902608 -> 1607676899248
	1607676902608 [label=ReluBackward0]
	1607676901936 -> 1607676902608
	1607676901936 [label=NativeBatchNormBackward0]
	1607676901552 -> 1607676901936
	1607676901552 [label=AddmmBackward0]
	1607677548752 -> 1607676901552
	1607675507248 [label="mlp.layers.14.bias
 (128)" fillcolor=lightblue]
	1607675507248 -> 1607677548752
	1607677548752 [label=AccumulateGrad]
	1607677544336 -> 1607676901552
	1607677544336 [label=ReluBackward0]
	1607677550000 -> 1607677544336
	1607677550000 [label=NativeBatchNormBackward0]
	1607677542800 -> 1607677550000
	1607677542800 [label=AddmmBackward0]
	1607677544432 -> 1607677542800
	1607675510608 [label="mlp.layers.11.bias
 (256)" fillcolor=lightblue]
	1607675510608 -> 1607677544432
	1607677544432 [label=AccumulateGrad]
	1607677544720 -> 1607677542800
	1607677544720 [label=ReluBackward0]
	1607677543568 -> 1607677544720
	1607677543568 [label=NativeBatchNormBackward0]
	1607677542560 -> 1607677543568
	1607677542560 [label=AddmmBackward0]
	1607677547744 -> 1607677542560
	1607675508784 [label="mlp.layers.7.bias
 (512)" fillcolor=lightblue]
	1607675508784 -> 1607677547744
	1607677547744 [label=AccumulateGrad]
	1607677547888 -> 1607677542560
	1607677547888 [label=ReluBackward0]
	1607677548512 -> 1607677547888
	1607677548512 [label=AddmmBackward0]
	1607677545872 -> 1607677548512
	1607675508592 [label="mlp.layers.4.bias
 (1024)" fillcolor=lightblue]
	1607675508592 -> 1607677545872
	1607677545872 [label=AccumulateGrad]
	1607677548320 -> 1607677548512
	1607677548320 [label=ReluBackward0]
	1607677548080 -> 1607677548320
	1607677548080 [label=AddmmBackward0]
	1607677548032 -> 1607677548080
	1607584111440 [label="mlp.layers.1.bias
 (2048)" fillcolor=lightblue]
	1607584111440 -> 1607677548032
	1607677548032 [label=AccumulateGrad]
	1607677545488 -> 1607677548080
	1607677545488 [label=NativeBatchNormBackward0]
	1607677547696 -> 1607677545488
	1607584110768 [label="mlp.layers.0.weight
 (1185)" fillcolor=lightblue]
	1607584110768 -> 1607677547696
	1607677547696 [label=AccumulateGrad]
	1607677546784 -> 1607677545488
	1607584114512 [label="mlp.layers.0.bias
 (1185)" fillcolor=lightblue]
	1607584114512 -> 1607677546784
	1607677546784 [label=AccumulateGrad]
	1607677544288 -> 1607677548080
	1607677544288 [label=TBackward0]
	1607677548176 -> 1607677544288
	1607584112208 [label="mlp.layers.1.weight
 (2048, 1185)" fillcolor=lightblue]
	1607584112208 -> 1607677548176
	1607677548176 [label=AccumulateGrad]
	1607677548608 -> 1607677548512
	1607677548608 [label=TBackward0]
	1607677547072 -> 1607677548608
	1607675500048 [label="mlp.layers.4.weight
 (1024, 2048)" fillcolor=lightblue]
	1607675500048 -> 1607677547072
	1607677547072 [label=AccumulateGrad]
	1607677543088 -> 1607677542560
	1607677543088 [label=TBackward0]
	1607677546880 -> 1607677543088
	1607675503984 [label="mlp.layers.7.weight
 (512, 1024)" fillcolor=lightblue]
	1607675503984 -> 1607677546880
	1607677546880 [label=AccumulateGrad]
	1607677543328 -> 1607677543568
	1607675496880 [label="mlp.layers.8.weight
 (512)" fillcolor=lightblue]
	1607675496880 -> 1607677543328
	1607677543328 [label=AccumulateGrad]
	1607677544960 -> 1607677543568
	1607675507056 [label="mlp.layers.8.bias
 (512)" fillcolor=lightblue]
	1607675507056 -> 1607677544960
	1607677544960 [label=AccumulateGrad]
	1607677544048 -> 1607677542800
	1607677544048 [label=TBackward0]
	1607677549424 -> 1607677544048
	1607675499760 [label="mlp.layers.11.weight
 (256, 512)" fillcolor=lightblue]
	1607675499760 -> 1607677549424
	1607677549424 [label=AccumulateGrad]
	1607677544480 -> 1607677550000
	1607675507920 [label="mlp.layers.12.weight
 (256)" fillcolor=lightblue]
	1607675507920 -> 1607677544480
	1607677544480 [label=AccumulateGrad]
	1607677542464 -> 1607677550000
	1607675510416 [label="mlp.layers.12.bias
 (256)" fillcolor=lightblue]
	1607675510416 -> 1607677542464
	1607677542464 [label=AccumulateGrad]
	1607677543040 -> 1607676901552
	1607677543040 [label=TBackward0]
	1607677544624 -> 1607677543040
	1607675508304 [label="mlp.layers.14.weight
 (128, 256)" fillcolor=lightblue]
	1607675508304 -> 1607677544624
	1607677544624 [label=AccumulateGrad]
	1607676893440 -> 1607676901936
	1607675495344 [label="mlp.layers.15.weight
 (128)" fillcolor=lightblue]
	1607675495344 -> 1607676893440
	1607676893440 [label=AccumulateGrad]
	1607677542704 -> 1607676901936
	1607675501968 [label="mlp.layers.15.bias
 (128)" fillcolor=lightblue]
	1607675501968 -> 1607677542704
	1607677542704 [label=AccumulateGrad]
	1607676899824 -> 1607676899248
	1607676899824 [label=TBackward0]
	1607676902992 -> 1607676899824
	1607675497936 [label="mlp.layers.17.weight
 (8, 128)" fillcolor=lightblue]
	1607675497936 -> 1607676902992
	1607676902992 [label=AccumulateGrad]
	1607676887104 -> 1607677615968
	1607676887104 [label=AddmmBackward0]
	1607676902512 -> 1607676887104
	1607584115856 [label="cnn.fc_layers.2.bias
 (8)" fillcolor=lightblue]
	1607584115856 -> 1607676902512
	1607676902512 [label=AccumulateGrad]
	1607677544912 -> 1607676887104
	1607677544912 [label=ReluBackward0]
	1607677542656 -> 1607677544912
	1607677542656 [label=AddmmBackward0]
	1607677542608 -> 1607677542656
	1607584116624 [label="cnn.fc_layers.0.bias
 (128)" fillcolor=lightblue]
	1607584116624 -> 1607677542608
	1607677542608 [label=AccumulateGrad]
	1607677547936 -> 1607677542656
	1607677547936 [label=ViewBackward0]
	1607677547216 -> 1607677547936
	1607677547216 [label=SqueezeBackward1]
	1607677547120 -> 1607677547216
	1607677547120 [label=MaxPool2DWithIndicesBackward0]
	1607677545584 -> 1607677547120
	1607677545584 [label=UnsqueezeBackward0]
	1607677546544 -> 1607677545584
	1607677546544 [label=ReluBackward0]
	1607677546400 -> 1607677546544
	1607677546400 [label=ConvolutionBackward0]
	1607677547600 -> 1607677546400
	1607677547600 [label=SqueezeBackward1]
	1607677542512 -> 1607677547600
	1607677542512 [label=MaxPool2DWithIndicesBackward0]
	1607677546256 -> 1607677542512
	1607677546256 [label=UnsqueezeBackward0]
	1607677545152 -> 1607677546256
	1607677545152 [label=ReluBackward0]
	1607677545920 -> 1607677545152
	1607677545920 [label=ConvolutionBackward0]
	1607677545392 -> 1607677545920
	1606162071728 [label="cnn.conv_layers.0.weight
 (16, 1, 3)" fillcolor=lightblue]
	1606162071728 -> 1607677545392
	1607677545392 [label=AccumulateGrad]
	1607677546592 -> 1607677545920
	1606162084496 [label="cnn.conv_layers.0.bias
 (16)" fillcolor=lightblue]
	1606162084496 -> 1607677546592
	1607677546592 [label=AccumulateGrad]
	1607677545344 -> 1607677546400
	1607584113552 [label="cnn.conv_layers.3.weight
 (32, 16, 3)" fillcolor=lightblue]
	1607584113552 -> 1607677545344
	1607677545344 [label=AccumulateGrad]
	1607677546352 -> 1607677546400
	1607584115088 [label="cnn.conv_layers.3.bias
 (32)" fillcolor=lightblue]
	1607584115088 -> 1607677546352
	1607677546352 [label=AccumulateGrad]
	1607677549856 -> 1607677542656
	1607677549856 [label=TBackward0]
	1607677547024 -> 1607677549856
	1607584115280 [label="cnn.fc_layers.0.weight
 (128, 9472)" fillcolor=lightblue]
	1607584115280 -> 1607677547024
	1607677547024 [label=AccumulateGrad]
	1607677548944 -> 1607676887104
	1607677548944 [label=TBackward0]
	1607677546640 -> 1607677548944
	1607584113456 [label="cnn.fc_layers.2.weight
 (8, 128)" fillcolor=lightblue]
	1607584113456 -> 1607677546640
	1607677546640 [label=AccumulateGrad]
	1607676900496 -> 1607677615968
	1607676900496 [label=AddmmBackward0]
	1607677544144 -> 1607676900496
	1607675507728 [label="rnn.fc.bias
 (8)" fillcolor=lightblue]
	1607675507728 -> 1607677544144
	1607677544144 [label=AccumulateGrad]
	1607677547168 -> 1607676900496
	1607677547168 [label=SliceBackward0]
	1607677542896 -> 1607677547168
	1607677542896 [label=SelectBackward0]
	1607677546160 -> 1607677542896
	1607677546160 [label=SliceBackward0]
	1607677543952 -> 1607677546160
	1607677543952 [label=CudnnRnnBackward0]
	1607677543520 -> 1607677543952
	1607675506480 [label="rnn.rnn.weight_ih_l0
 (128, 1185)" fillcolor=lightblue]
	1607675506480 -> 1607677543520
	1607677543520 [label=AccumulateGrad]
	1607677545056 -> 1607677543952
	1607675500336 [label="rnn.rnn.weight_hh_l0
 (128, 128)" fillcolor=lightblue]
	1607675500336 -> 1607677545056
	1607677545056 [label=AccumulateGrad]
	1607677544576 -> 1607677543952
	1607675506288 [label="rnn.rnn.bias_ih_l0
 (128)" fillcolor=lightblue]
	1607675506288 -> 1607677544576
	1607677544576 [label=AccumulateGrad]
	1607677544816 -> 1607677543952
	1607675510512 [label="rnn.rnn.bias_hh_l0
 (128)" fillcolor=lightblue]
	1607675510512 -> 1607677544816
	1607677544816 [label=AccumulateGrad]
	1607677544240 -> 1607676900496
	1607677544240 [label=TBackward0]
	1607677543664 -> 1607677544240
	1607675506672 [label="rnn.fc.weight
 (8, 128)" fillcolor=lightblue]
	1607675506672 -> 1607677543664
	1607677543664 [label=AccumulateGrad]
	1607676899536 -> 1607677615968
	1607676899536 [label=AddmmBackward0]
	1607677544000 -> 1607676899536
	1607675502160 [label="lstm.fc.bias
 (8)" fillcolor=lightblue]
	1607675502160 -> 1607677544000
	1607677544000 [label=AccumulateGrad]
	1607677546736 -> 1607676899536
	1607677546736 [label=SliceBackward0]
	1607677547504 -> 1607677546736
	1607677547504 [label=SelectBackward0]
	1607677545824 -> 1607677547504
	1607677545824 [label=SliceBackward0]
	1607677543424 -> 1607677545824
	1607677543424 [label=CudnnRnnBackward0]
	1607677543184 -> 1607677543424
	1607675498896 [label="lstm.lstm.weight_ih_l0
 (512, 1185)" fillcolor=lightblue]
	1607675498896 -> 1607677543184
	1607677543184 [label=AccumulateGrad]
	1607677543376 -> 1607677543424
	1607675500624 [label="lstm.lstm.weight_hh_l0
 (512, 128)" fillcolor=lightblue]
	1607675500624 -> 1607677543376
	1607677543376 [label=AccumulateGrad]
	1607677546112 -> 1607677543424
	1607675497168 [label="lstm.lstm.bias_ih_l0
 (512)" fillcolor=lightblue]
	1607675497168 -> 1607677546112
	1607677546112 [label=AccumulateGrad]
	1607677542848 -> 1607677543424
	1607675500720 [label="lstm.lstm.bias_hh_l0
 (512)" fillcolor=lightblue]
	1607675500720 -> 1607677542848
	1607677542848 [label=AccumulateGrad]
	1607677547264 -> 1607676899536
	1607677547264 [label=TBackward0]
	1607677545632 -> 1607677547264
	1607675499856 [label="lstm.fc.weight
 (8, 128)" fillcolor=lightblue]
	1607675499856 -> 1607677545632
	1607677545632 [label=AccumulateGrad]
	1607677611552 -> 1607677617360
	1607677611552 [label=TBackward0]
	1607676899632 -> 1607677611552
	1607675498032 [label="fc.weight
 (8, 32)" fillcolor=lightblue]
	1607675498032 -> 1607676899632
	1607676899632 [label=AccumulateGrad]
	1607677617360 -> 1607677603248
}

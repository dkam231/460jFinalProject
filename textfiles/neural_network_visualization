digraph {
	graph [size="18.45,18.45"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1607583709520 [label="
 (1, 8)" fillcolor=darkolivegreen1]
	1606161135216 [label=AddmmBackward0]
	1606161132864 -> 1606161135216
	1606162086704 [label="layers.17.bias
 (8)" fillcolor=lightblue]
	1606162086704 -> 1606161132864
	1606161132864 [label=AccumulateGrad]
	1606161126240 -> 1606161135216
	1606161126240 [label=LeakyReluBackward0]
	1606161129456 -> 1606161126240
	1606161129456 [label=NativeBatchNormBackward0]
	1606161134640 -> 1606161129456
	1606161134640 [label=AddmmBackward0]
	1607583392352 -> 1606161134640
	1606162087376 [label="layers.14.bias
 (128)" fillcolor=lightblue]
	1606162087376 -> 1607583392352
	1607583392352 [label=AccumulateGrad]
	1607583392304 -> 1606161134640
	1607583392304 [label=LeakyReluBackward0]
	1607583384000 -> 1607583392304
	1607583384000 [label=NativeBatchNormBackward0]
	1607583386208 -> 1607583384000
	1607583386208 [label=AddmmBackward0]
	1607583389136 -> 1607583386208
	1606154232048 [label="layers.11.bias
 (256)" fillcolor=lightblue]
	1606154232048 -> 1607583389136
	1607583389136 [label=AccumulateGrad]
	1607583389472 -> 1607583386208
	1607583389472 [label=LeakyReluBackward0]
	1607583387264 -> 1607583389472
	1607583387264 [label=NativeBatchNormBackward0]
	1607583384912 -> 1607583387264
	1607583384912 [label=AddmmBackward0]
	1607583390864 -> 1607583384912
	1606155274960 [label="layers.7.bias
 (512)" fillcolor=lightblue]
	1606155274960 -> 1607583390864
	1607583390864 [label=AccumulateGrad]
	1607583385296 -> 1607583384912
	1607583385296 [label=LeakyReluBackward0]
	1607583389568 -> 1607583385296
	1607583389568 [label=AddmmBackward0]
	1607583391344 -> 1607583389568
	1606159601296 [label="layers.4.bias
 (1024)" fillcolor=lightblue]
	1606159601296 -> 1607583391344
	1607583391344 [label=AccumulateGrad]
	1607583388032 -> 1607583389568
	1607583388032 [label=LeakyReluBackward0]
	1607583389856 -> 1607583388032
	1607583389856 [label=AddmmBackward0]
	1607583390432 -> 1607583389856
	1606159601488 [label="layers.1.bias
 (2048)" fillcolor=lightblue]
	1606159601488 -> 1607583390432
	1607583390432 [label=AccumulateGrad]
	1607583389232 -> 1607583389856
	1607583389232 [label=NativeBatchNormBackward0]
	1607583390144 -> 1607583389232
	1606159602544 [label="layers.0.weight
 (1185)" fillcolor=lightblue]
	1606159602544 -> 1607583390144
	1607583390144 [label=AccumulateGrad]
	1607583390912 -> 1607583389232
	1606159601872 [label="layers.0.bias
 (1185)" fillcolor=lightblue]
	1606159601872 -> 1607583390912
	1607583390912 [label=AccumulateGrad]
	1607583388944 -> 1607583389856
	1607583388944 [label=TBackward0]
	1607583389808 -> 1607583388944
	1606159601680 [label="layers.1.weight
 (2048, 1185)" fillcolor=lightblue]
	1606159601680 -> 1607583389808
	1607583389808 [label=AccumulateGrad]
	1607583391104 -> 1607583389568
	1607583391104 [label=TBackward0]
	1607583390048 -> 1607583391104
	1606159601392 [label="layers.4.weight
 (1024, 2048)" fillcolor=lightblue]
	1606159601392 -> 1607583390048
	1607583390048 [label=AccumulateGrad]
	1607583387504 -> 1607583384912
	1607583387504 [label=TBackward0]
	1607583390096 -> 1607583387504
	1606159601200 [label="layers.7.weight
 (512, 1024)" fillcolor=lightblue]
	1606159601200 -> 1607583390096
	1607583390096 [label=AccumulateGrad]
	1607583385680 -> 1607583387264
	1606155276400 [label="layers.8.weight
 (512)" fillcolor=lightblue]
	1606155276400 -> 1607583385680
	1607583385680 [label=AccumulateGrad]
	1607583387840 -> 1607583387264
	1606155276592 [label="layers.8.bias
 (512)" fillcolor=lightblue]
	1606155276592 -> 1607583387840
	1607583387840 [label=AccumulateGrad]
	1607583387072 -> 1607583386208
	1607583387072 [label=TBackward0]
	1607583390480 -> 1607583387072
	1606154232432 [label="layers.11.weight
 (256, 512)" fillcolor=lightblue]
	1606154232432 -> 1607583390480
	1607583390480 [label=AccumulateGrad]
	1607583386448 -> 1607583384000
	1606154231664 [label="layers.12.weight
 (256)" fillcolor=lightblue]
	1606154231664 -> 1607583386448
	1607583386448 [label=AccumulateGrad]
	1607583387456 -> 1607583384000
	1606154231952 [label="layers.12.bias
 (256)" fillcolor=lightblue]
	1606154231952 -> 1607583387456
	1607583387456 [label=AccumulateGrad]
	1607583391632 -> 1606161134640
	1607583391632 [label=TBackward0]
	1607583388272 -> 1607583391632
	1606162087472 [label="layers.14.weight
 (128, 256)" fillcolor=lightblue]
	1606162087472 -> 1607583388272
	1607583388272 [label=AccumulateGrad]
	1606161134688 -> 1606161129456
	1606162087280 [label="layers.15.weight
 (128)" fillcolor=lightblue]
	1606162087280 -> 1606161134688
	1606161134688 [label=AccumulateGrad]
	1606161127440 -> 1606161129456
	1606162087184 [label="layers.15.bias
 (128)" fillcolor=lightblue]
	1606162087184 -> 1606161127440
	1606161127440 [label=AccumulateGrad]
	1606161127104 -> 1606161135216
	1606161127104 [label=TBackward0]
	1606161127200 -> 1606161127104
	1606162086800 [label="layers.17.weight
 (8, 128)" fillcolor=lightblue]
	1606162086800 -> 1606161127200
	1606161127200 [label=AccumulateGrad]
	1606161135216 -> 1607583709520
}
